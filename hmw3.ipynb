{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import pysindy as sindy\n",
    "\n",
    "data = scipy.io.loadmat('DataHw3Q1.mat')\n",
    "\n",
    "data.keys()\n",
    "\n",
    "t = data['t'].T\n",
    "x1 =data['x'][:,0]\n",
    "x2 = data['x'][:,1]\n",
    "\n",
    "nt = x1.shape[0]\n",
    "split = int(0.8*nt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ != \"testing\":\n",
    "  t  = t.reshape(-1)[:split]\n",
    "  x1 = x1[:split]\n",
    "  x2 = x2[:split]\n",
    "else:\n",
    "  t  = t[split:]\n",
    "  x1 = x1[split:]\n",
    "  x2 = x2[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data matrix\n",
    "data = np.vstack([x1, x2]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the library of potential functions\n",
    "\n",
    "differentiation_method = sindy.FiniteDifference(order=2)\n",
    "\n",
    "feature_library = sindy.PolynomialLibrary(degree=3)\n",
    "\n",
    "feature_names = [\"x\", \"y\"]\n",
    "\n",
    "optimizer = sindy.STLSQ(threshold = 0.1)\n",
    "\n",
    "model = sindy.SINDy(\n",
    "    differentiation_method=differentiation_method,\n",
    "    feature_library= feature_library,\n",
    "    optimizer= optimizer,\n",
    "    feature_names = feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The required system of equations:\n",
      "(x)' = 0.499 1 + 0.998 x + -0.998 y + -0.333 x^3\n",
      "(y)' = 0.200 x + -0.160 y\n"
     ]
    }
   ],
   "source": [
    "model.fit(data, t=t)\n",
    "print(\"The required system of equations:\")\n",
    "model.print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2: Encoder vs DMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras.backend as K\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational AutoEncoder for Compressed Sensing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 32 #Height\n",
    "W = 32 #Width\n",
    "C = 1  #Channels\n",
    "L = 2 #Latent dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The way to complie z_mean and z_log_var\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))  # standarnd normal distribution as default, with mean=0 and std=1.0\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try larger strides and kernel and more CNN layer to make sure when it is flatten the parameter wont be too much !\n",
    "def Encoder_VAE():\n",
    "    encoder_inputs = keras.Input(name=\"Encoder_inputs\",shape = (H,W,C))\n",
    "\n",
    "    x = layers.Conv2D(32,(3,3),strides=(2,2),padding =\"same\",name= \"conv1\")(encoder_inputs)\n",
    "\n",
    "\n",
    "    x = layers.Conv2D(64,(3,3),strides=(2,2),padding =\"same\",name= \"conv2\")(x)\n",
    "\n",
    "\n",
    "    #x = layers.Conv2D(64,(3,3),strides=(2,2),padding =\"same\",name= \"conv3\")(x)\n",
    "\n",
    "\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(16,activation = \"relu\")(x) #apply relu activation function\n",
    "    z_mean = layers.Dense(L,name= \"z_mean\")(x)\n",
    "    z_log_var = layers.Dense(L,name=\"z_log_var\")(x)\n",
    "\n",
    "    z = layers.Lambda(sampling, output_shape=(L,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "    return keras.Model(inputs =encoder_inputs,outputs=[z_mean,z_log_var],name=\"encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " Encoder_inputs (InputLayer  [(None, 32, 32, 1)]          0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv1 (Conv2D)              (None, 16, 16, 32)           320       ['Encoder_inputs[0][0]']      \n",
      "                                                                                                  \n",
      " conv2 (Conv2D)              (None, 8, 8, 64)             18496     ['conv1[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 4096)                 0         ['conv2[0][0]']               \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 16)                   65552     ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " z_mean (Dense)              (None, 2)                    34        ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " z_log_var (Dense)           (None, 2)                    34        ['dense[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 84436 (329.83 KB)\n",
      "Trainable params: 84436 (329.83 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Encoder = Encoder_VAE()\n",
    "Encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler(layers.Layer):\n",
    " def call(self,z_mean,z_log_var):\n",
    "    batch_size = tf.shape(z_mean)[0]\n",
    "    z_size = tf.shape(z_mean)[1]\n",
    "    epsilon = tf.random.normal(shape=(batch_size,z_size))\n",
    "\n",
    "    return z_mean + tf.exp(0.5*z_log_var)*epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder: Mapping the the latent space, of dimension 2, into images in this case, flow modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Decoder_VAE():\n",
    "  latent_inputs= layers.Input(shape=(L,),name=\"latent_input\")\n",
    "\n",
    "  x= layers.Dense(8*8*64,activation=\"relu\",name=\"decoder_dense1\")(latent_inputs)\n",
    "\n",
    "  x = layers.Reshape((8,8,64))(x)\n",
    "\n",
    "  x = layers.Conv2DTranspose(64,(3,3),strides=(2,2),padding=\"same\",activation=\"relu\",name=\"decoder_1\")(x)\n",
    "  x = layers.Conv2DTranspose(32,(3,3),strides=(2,2),padding=\"same\",activation=\"relu\",name=\"decoder_2\")(x)\n",
    "\n",
    "  #x = layers.Conv2DTranspose(32,(5,5),strides=(2,2),padding=\"same\",activation=\"relu\",name=\"decoder_3\")(x)\n",
    "\n",
    "\n",
    "\n",
    "  decoder_outputs = layers.Conv2D(C,(3,3),padding=\"same\")(x)\n",
    "  return keras.Model(latent_inputs,decoder_outputs,name=\"decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " latent_input (InputLayer)   [(None, 2)]               0         \n",
      "                                                                 \n",
      " decoder_dense1 (Dense)      (None, 4096)              12288     \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " decoder_1 (Conv2DTranspose  (None, 16, 16, 64)        36928     \n",
      " )                                                               \n",
      "                                                                 \n",
      " decoder_2 (Conv2DTranspose  (None, 32, 32, 32)        18464     \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 32, 32, 1)         289       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 67969 (265.50 KB)\n",
      "Trainable params: 67969 (265.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Decoder = Decoder_VAE()\n",
    "Decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "  def __init__(self,encoder,decoder,**kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.encoder = Encoder\n",
    "    self.decoder = Decoder\n",
    "    self.sampler = Sampler()\n",
    "    self.total_loss_tracker = keras.metrics.Mean(name=\"total_ oss\")\n",
    "    self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction loss\")\n",
    "    self.kl_loss_tracker = keras.metrics.Mean(name=\"kl loss\")\n",
    "\n",
    "  @property\n",
    "  def metrics(self):\n",
    "    return [\n",
    "        self.reconstruction_loss_tracker,\n",
    "        self.kl_loss_tracker\n",
    "        ]\n",
    "\n",
    "  def train_step(self,data):\n",
    "    # Gradient Descent Back Propagation\n",
    "    with tf.GradientTape() as tape:\n",
    "      z_mean,z_log_var = self.encoder(data)\n",
    "      z = self.sampler(z_mean,z_log_var)\n",
    "      reconstruction = Decoder(z)\n",
    "\n",
    "      reconstruction_loss = tf.reduce_mean(\n",
    "                          tf.reduce_sum(\n",
    "                              keras.losses.mae(data, reconstruction),\n",
    "                              axis = (1,2))\n",
    "                          )\n",
    "\n",
    "      kl_loss = -0.5 * (1 + z_log_var -tf.square(z_mean) - tf.exp(z_log_var) )\n",
    "\n",
    "      total_loss = reconstruction_loss + tf.reduce_mean(kl_loss)\n",
    "    grads = tape.gradient(total_loss,self.trainable_weights)\n",
    "\n",
    "    # Update gradients descent for optimize the parameters\n",
    "    self.optimizer.apply_gradients( zip(grads,self.trainable_weights) )\n",
    "\n",
    "    self.total_loss_tracker.update_state(total_loss)\n",
    "    self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "    self.kl_loss_tracker.update_state(kl_loss)\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"total_loss\":self.total_loss_tracker.result(),\n",
    "        \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "        \"kl_loss\": self.kl_loss_tracker.result()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data(N):\n",
    "    y = np.linspace(-2,2,H) # spatial coordinate\n",
    "    Ny = np.size(y)\n",
    "\n",
    "    amp1 = 1\n",
    "    y01 = 0.5\n",
    "    sigmay1 = 0.6\n",
    "\n",
    "    amp2 = 1.2\n",
    "    y02 = -0.5\n",
    "    sigmay2 = 0.3\n",
    "\n",
    "    dt = 0.1\n",
    "    Nt = 32\n",
    "    tend = dt*(Nt-1)\n",
    "    t = np.linspace(0,tend,Nt) # time\n",
    "    X_all = np.zeros(shape=(N,Ny,Nt))\n",
    "\n",
    "    for i in range(N):\n",
    "\n",
    "        omega1 = np.random.randint(1,2,size=1)+np.random.rand(1)\n",
    "\n",
    "        omega2 = np.random.randint(4,5,size=1)+np.random.rand(1)\n",
    "\n",
    "        v1 = amp1*np.exp(-((y-y01)**2)/(2*sigmay1**2))\n",
    "        v2 = amp2*np.exp(-((y-y02)**2)/(2*sigmay2**2))\n",
    "\n",
    "        X = np.zeros([Ny,Nt],dtype=complex)\n",
    "        for tt in range(Nt):\n",
    "            X[:,tt] = v1*np.exp(1j*omega1*t[tt])+v2*np.exp(1j*omega2*t[tt]) \n",
    "                \n",
    "        X = (X-X.min())/(X.max()-X.min())\n",
    "        X_all[i,:,:] = X\n",
    "    \n",
    "    X_all = np.expand_dims(X_all,axis=-1)\n",
    "\n",
    "    return X_all\n",
    "\n",
    "############################################################################################\n",
    "# -------------------- 2D CONTOURS\n",
    "############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_519838/937327612.py:33: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  X_all[i,:,:] = X\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Dataset is (10000, 32, 32, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f493dcd7100>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = Data(10000)\n",
    "print(f\"Shape of Dataset is {data.shape}\")\n",
    "\n",
    "imh = plt.imshow(data[0,:,:,0],cmap=\"jet\")\n",
    "plt.colorbar(imh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "encoder = Encoder_VAE()\n",
    "decoder = Decoder_VAE()\n",
    "\n",
    "vae = VAE(encoder,decoder)\n",
    "\n",
    "vae.compile(optimizer=keras.optimizers.Adam(0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "  if lr > 1e-4:\n",
    "    return lr * 0.0001**(epoch/5000)\n",
    "  else:\n",
    "    return lr\n",
    "\n",
    "callbacks = [keras.callbacks.LearningRateScheduler(scheduler)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "157/157 [==============================] - 12s 65ms/step - total_loss: 167.0984 - reconstruction_loss: 162.5018 - kl_loss: 4.5966\n",
      "Epoch 2/100\n",
      "157/157 [==============================] - 10s 63ms/step - total_loss: 71.3656 - reconstruction_loss: 31.1909 - kl_loss: 2.5901\n",
      "Epoch 3/100\n",
      "157/157 [==============================] - 10s 63ms/step - total_loss: 50.0901 - reconstruction_loss: 12.8385 - kl_loss: 3.0584\n",
      "Epoch 4/100\n",
      "157/157 [==============================] - 9s 56ms/step - total_loss: 39.8457 - reconstruction_loss: 10.7013 - kl_loss: 3.1166\n",
      "Epoch 5/100\n",
      "157/157 [==============================] - 9s 56ms/step - total_loss: 33.3730 - reconstruction_loss: 6.9976 - kl_loss: 3.2923\n",
      "Epoch 6/100\n",
      "157/157 [==============================] - 9s 55ms/step - total_loss: 29.2199 - reconstruction_loss: 7.0226 - kl_loss: 3.3004\n",
      "Epoch 7/100\n",
      "157/157 [==============================] - 9s 56ms/step - total_loss: 26.5415 - reconstruction_loss: 9.4174 - kl_loss: 3.2007\n",
      "Epoch 8/100\n",
      "157/157 [==============================] - 9s 54ms/step - total_loss: 24.3823 - reconstruction_loss: 5.7738 - kl_loss: 3.3395\n",
      "Epoch 9/100\n",
      "157/157 [==============================] - 9s 55ms/step - total_loss: 22.5982 - reconstruction_loss: 5.9960 - kl_loss: 3.3045\n",
      "Epoch 10/100\n",
      "157/157 [==============================] - 8s 53ms/step - total_loss: 21.2357 - reconstruction_loss: 5.4146 - kl_loss: 3.3511\n",
      "Epoch 11/100\n",
      "157/157 [==============================] - 8s 51ms/step - total_loss: 20.1054 - reconstruction_loss: 6.9560 - kl_loss: 3.2554\n",
      "Epoch 12/100\n",
      "157/157 [==============================] - 9s 58ms/step - total_loss: 19.1417 - reconstruction_loss: 5.8409 - kl_loss: 3.3123\n",
      "Epoch 13/100\n",
      "157/157 [==============================] - 9s 55ms/step - total_loss: 18.3282 - reconstruction_loss: 5.6295 - kl_loss: 3.3006\n",
      "Epoch 14/100\n",
      "157/157 [==============================] - 9s 55ms/step - total_loss: 17.6261 - reconstruction_loss: 5.3456 - kl_loss: 3.3267\n",
      "Epoch 15/100\n",
      "157/157 [==============================] - 9s 57ms/step - total_loss: 17.0709 - reconstruction_loss: 5.3414 - kl_loss: 3.2832\n",
      "Epoch 16/100\n",
      "157/157 [==============================] - 9s 56ms/step - total_loss: 16.5376 - reconstruction_loss: 5.7849 - kl_loss: 3.2686\n",
      "Epoch 17/100\n",
      "157/157 [==============================] - 8s 54ms/step - total_loss: 16.0682 - reconstruction_loss: 6.0380 - kl_loss: 3.2492\n",
      "Epoch 18/100\n",
      "157/157 [==============================] - 9s 55ms/step - total_loss: 15.6265 - reconstruction_loss: 5.1232 - kl_loss: 3.2606\n",
      "Epoch 19/100\n",
      "157/157 [==============================] - 10s 62ms/step - total_loss: 15.2597 - reconstruction_loss: 5.8221 - kl_loss: 3.2376\n",
      "Epoch 20/100\n",
      "157/157 [==============================] - 10s 65ms/step - total_loss: 14.9276 - reconstruction_loss: 5.1321 - kl_loss: 3.2774\n",
      "Epoch 21/100\n",
      "157/157 [==============================] - 21s 132ms/step - total_loss: 14.6769 - reconstruction_loss: 7.8727 - kl_loss: 3.1308\n",
      "Epoch 22/100\n",
      "157/157 [==============================] - 21s 131ms/step - total_loss: 14.4209 - reconstruction_loss: 4.8171 - kl_loss: 3.2773\n",
      "Epoch 23/100\n",
      "157/157 [==============================] - 21s 132ms/step - total_loss: 14.1852 - reconstruction_loss: 6.5975 - kl_loss: 3.1636\n",
      "Epoch 24/100\n",
      "157/157 [==============================] - 20s 130ms/step - total_loss: 13.9395 - reconstruction_loss: 5.0437 - kl_loss: 3.2543\n",
      "Epoch 25/100\n",
      "157/157 [==============================] - 20s 130ms/step - total_loss: 13.7177 - reconstruction_loss: 5.6208 - kl_loss: 3.2112\n",
      "Epoch 26/100\n",
      "157/157 [==============================] - 21s 132ms/step - total_loss: 13.5106 - reconstruction_loss: 5.1701 - kl_loss: 3.2459\n",
      "Epoch 27/100\n",
      "157/157 [==============================] - 21s 131ms/step - total_loss: 13.3617 - reconstruction_loss: 6.5860 - kl_loss: 3.1560\n",
      "Epoch 28/100\n",
      "157/157 [==============================] - 21s 133ms/step - total_loss: 13.1825 - reconstruction_loss: 4.8920 - kl_loss: 3.2086\n",
      "Epoch 29/100\n",
      "157/157 [==============================] - 20s 130ms/step - total_loss: 13.0008 - reconstruction_loss: 4.7075 - kl_loss: 3.2788\n",
      "Epoch 30/100\n",
      "157/157 [==============================] - 21s 132ms/step - total_loss: 12.8373 - reconstruction_loss: 4.9208 - kl_loss: 3.2421\n",
      "Epoch 31/100\n",
      "157/157 [==============================] - 20s 130ms/step - total_loss: 12.6916 - reconstruction_loss: 5.1254 - kl_loss: 3.2345\n",
      "Epoch 32/100\n",
      "157/157 [==============================] - 20s 130ms/step - total_loss: 12.5472 - reconstruction_loss: 5.0892 - kl_loss: 3.1973\n",
      "Epoch 33/100\n",
      "157/157 [==============================] - 21s 133ms/step - total_loss: 12.4230 - reconstruction_loss: 5.6009 - kl_loss: 3.1969\n",
      "Epoch 34/100\n",
      "157/157 [==============================] - 21s 133ms/step - total_loss: 12.3077 - reconstruction_loss: 4.8013 - kl_loss: 3.2601\n",
      "Epoch 35/100\n",
      "157/157 [==============================] - 21s 133ms/step - total_loss: 12.1977 - reconstruction_loss: 5.4807 - kl_loss: 3.1768\n",
      "Epoch 36/100\n",
      "157/157 [==============================] - 21s 131ms/step - total_loss: 12.0795 - reconstruction_loss: 4.8247 - kl_loss: 3.2187\n",
      "Epoch 37/100\n",
      "157/157 [==============================] - 21s 133ms/step - total_loss: 12.0036 - reconstruction_loss: 6.7274 - kl_loss: 3.0948\n",
      "Epoch 38/100\n",
      "157/157 [==============================] - 21s 131ms/step - total_loss: 11.9409 - reconstruction_loss: 6.6912 - kl_loss: 3.1244\n",
      "Epoch 39/100\n",
      "157/157 [==============================] - 21s 132ms/step - total_loss: 11.8590 - reconstruction_loss: 5.0860 - kl_loss: 3.1776\n",
      "Epoch 40/100\n",
      "157/157 [==============================] - 21s 132ms/step - total_loss: 11.7563 - reconstruction_loss: 4.4568 - kl_loss: 3.2384\n",
      "Epoch 41/100\n",
      "157/157 [==============================] - 21s 131ms/step - total_loss: 11.6653 - reconstruction_loss: 4.5861 - kl_loss: 3.2175\n",
      "Epoch 42/100\n",
      "157/157 [==============================] - 21s 131ms/step - total_loss: 11.5855 - reconstruction_loss: 5.6335 - kl_loss: 3.1712\n",
      "Epoch 43/100\n",
      "157/157 [==============================] - 21s 131ms/step - total_loss: 11.5117 - reconstruction_loss: 4.3401 - kl_loss: 3.2808\n",
      "Epoch 44/100\n",
      "157/157 [==============================] - 21s 132ms/step - total_loss: 11.4242 - reconstruction_loss: 4.3615 - kl_loss: 3.2501\n",
      "Epoch 45/100\n",
      "157/157 [==============================] - 21s 131ms/step - total_loss: 11.3424 - reconstruction_loss: 4.7469 - kl_loss: 3.2289\n",
      "Epoch 46/100\n",
      "157/157 [==============================] - 21s 132ms/step - total_loss: 11.2662 - reconstruction_loss: 4.7118 - kl_loss: 3.2333\n",
      "Epoch 47/100\n",
      "157/157 [==============================] - 21s 131ms/step - total_loss: 11.2138 - reconstruction_loss: 5.9424 - kl_loss: 3.1582\n",
      "Epoch 48/100\n",
      "157/157 [==============================] - 20s 130ms/step - total_loss: 11.1534 - reconstruction_loss: 4.6401 - kl_loss: 3.2210\n",
      "Epoch 49/100\n",
      "157/157 [==============================] - 20s 130ms/step - total_loss: 11.0975 - reconstruction_loss: 5.1700 - kl_loss: 3.1776\n",
      "Epoch 50/100\n",
      "157/157 [==============================] - 21s 132ms/step - total_loss: 11.0330 - reconstruction_loss: 4.6436 - kl_loss: 3.2255\n",
      "Epoch 51/100\n",
      "157/157 [==============================] - 21s 133ms/step - total_loss: 10.9692 - reconstruction_loss: 4.7784 - kl_loss: 3.2145\n",
      "Epoch 52/100\n",
      "157/157 [==============================] - 21s 132ms/step - total_loss: 10.9142 - reconstruction_loss: 5.7441 - kl_loss: 3.1724\n",
      "Epoch 53/100\n",
      "157/157 [==============================] - 21s 133ms/step - total_loss: 10.8512 - reconstruction_loss: 4.3406 - kl_loss: 3.2632\n",
      "Epoch 54/100\n",
      "157/157 [==============================] - 21s 132ms/step - total_loss: 10.7915 - reconstruction_loss: 4.4483 - kl_loss: 3.2151\n",
      "Epoch 55/100\n",
      "157/157 [==============================] - 21s 131ms/step - total_loss: 10.7362 - reconstruction_loss: 4.5259 - kl_loss: 3.2245\n",
      "Epoch 56/100\n",
      "157/157 [==============================] - 21s 131ms/step - total_loss: 10.6889 - reconstruction_loss: 5.0234 - kl_loss: 3.1632\n",
      "Epoch 57/100\n",
      "157/157 [==============================] - 21s 132ms/step - total_loss: 10.6385 - reconstruction_loss: 4.7029 - kl_loss: 3.2133\n",
      "Epoch 58/100\n",
      "157/157 [==============================] - 21s 134ms/step - total_loss: 10.5916 - reconstruction_loss: 4.8117 - kl_loss: 3.2006\n",
      "Epoch 59/100\n",
      "157/157 [==============================] - 21s 132ms/step - total_loss: 10.5406 - reconstruction_loss: 4.3468 - kl_loss: 3.2495\n",
      "Epoch 60/100\n",
      "157/157 [==============================] - 21s 133ms/step - total_loss: 10.4975 - reconstruction_loss: 5.0022 - kl_loss: 3.2015\n",
      "Epoch 61/100\n",
      "157/157 [==============================] - 21s 132ms/step - total_loss: 10.4612 - reconstruction_loss: 4.9978 - kl_loss: 3.2085\n",
      "Epoch 62/100\n",
      "157/157 [==============================] - 21s 132ms/step - total_loss: 10.4205 - reconstruction_loss: 4.2894 - kl_loss: 3.2466\n",
      "Epoch 63/100\n",
      "157/157 [==============================] - 21s 132ms/step - total_loss: 10.3784 - reconstruction_loss: 4.6840 - kl_loss: 3.1937\n",
      "Epoch 64/100\n",
      "157/157 [==============================] - 21s 133ms/step - total_loss: 10.3433 - reconstruction_loss: 5.4997 - kl_loss: 3.1652\n",
      "Epoch 65/100\n",
      "157/157 [==============================] - 21s 132ms/step - total_loss: 10.3121 - reconstruction_loss: 5.3465 - kl_loss: 3.1644\n",
      "Epoch 66/100\n",
      "157/157 [==============================] - 21s 133ms/step - total_loss: 10.2818 - reconstruction_loss: 4.4217 - kl_loss: 3.2313\n",
      "Epoch 67/100\n",
      "157/157 [==============================] - 21s 133ms/step - total_loss: 10.2414 - reconstruction_loss: 4.1611 - kl_loss: 3.2313\n",
      "Epoch 68/100\n",
      "157/157 [==============================] - 21s 132ms/step - total_loss: 10.2002 - reconstruction_loss: 4.2696 - kl_loss: 3.2283\n",
      "Epoch 69/100\n",
      "157/157 [==============================] - 21s 132ms/step - total_loss: 10.1648 - reconstruction_loss: 4.5696 - kl_loss: 3.2278\n",
      "Epoch 70/100\n",
      "157/157 [==============================] - 21s 133ms/step - total_loss: 10.1297 - reconstruction_loss: 4.3418 - kl_loss: 3.2352\n",
      "Epoch 71/100\n",
      "157/157 [==============================] - 21s 132ms/step - total_loss: 10.1053 - reconstruction_loss: 5.4024 - kl_loss: 3.1301\n",
      "Epoch 72/100\n",
      "157/157 [==============================] - 21s 132ms/step - total_loss: 10.0694 - reconstruction_loss: 4.1468 - kl_loss: 3.2382\n",
      "Epoch 73/100\n",
      "157/157 [==============================] - 21s 132ms/step - total_loss: 10.0457 - reconstruction_loss: 5.9746 - kl_loss: 3.1216\n",
      "Epoch 74/100\n",
      "157/157 [==============================] - 21s 133ms/step - total_loss: 10.0182 - reconstruction_loss: 4.3390 - kl_loss: 3.2061\n",
      "Epoch 75/100\n",
      "157/157 [==============================] - 21s 132ms/step - total_loss: 9.9863 - reconstruction_loss: 4.5893 - kl_loss: 3.1926\n",
      "Epoch 76/100\n",
      "157/157 [==============================] - 21s 132ms/step - total_loss: 9.9610 - reconstruction_loss: 4.9436 - kl_loss: 3.2019\n",
      "Epoch 77/100\n",
      "157/157 [==============================] - 21s 133ms/step - total_loss: 9.9296 - reconstruction_loss: 3.9995 - kl_loss: 3.2483\n",
      "Epoch 78/100\n",
      "157/157 [==============================] - 21s 131ms/step - total_loss: 9.8963 - reconstruction_loss: 4.2156 - kl_loss: 3.2090\n",
      "Epoch 79/100\n",
      "157/157 [==============================] - 21s 133ms/step - total_loss: 9.8761 - reconstruction_loss: 5.6121 - kl_loss: 3.1137\n",
      "Epoch 80/100\n",
      "157/157 [==============================] - 21s 133ms/step - total_loss: 9.8520 - reconstruction_loss: 5.0350 - kl_loss: 3.1571\n",
      "Epoch 81/100\n",
      "157/157 [==============================] - 21s 131ms/step - total_loss: 9.8308 - reconstruction_loss: 5.5583 - kl_loss: 3.1297\n",
      "Epoch 82/100\n",
      "157/157 [==============================] - 20s 130ms/step - total_loss: 9.8075 - reconstruction_loss: 4.6944 - kl_loss: 3.1973\n",
      "Epoch 83/100\n",
      "157/157 [==============================] - 21s 131ms/step - total_loss: 9.7801 - reconstruction_loss: 4.0309 - kl_loss: 3.2531\n",
      "Epoch 84/100\n",
      "157/157 [==============================] - 21s 131ms/step - total_loss: 9.7510 - reconstruction_loss: 4.2443 - kl_loss: 3.2195\n",
      "Epoch 85/100\n",
      "157/157 [==============================] - 21s 134ms/step - total_loss: 9.7313 - reconstruction_loss: 5.6050 - kl_loss: 3.1334\n",
      "Epoch 86/100\n",
      "157/157 [==============================] - 21s 133ms/step - total_loss: 9.7113 - reconstruction_loss: 4.5517 - kl_loss: 3.1941\n",
      "Epoch 87/100\n",
      "157/157 [==============================] - 21s 131ms/step - total_loss: 9.6844 - reconstruction_loss: 3.8496 - kl_loss: 3.2522\n",
      "Epoch 88/100\n",
      "157/157 [==============================] - 21s 131ms/step - total_loss: 9.6555 - reconstruction_loss: 4.0399 - kl_loss: 3.2237\n",
      "Epoch 89/100\n",
      "157/157 [==============================] - 21s 132ms/step - total_loss: 9.6315 - reconstruction_loss: 4.7672 - kl_loss: 3.1889\n",
      "Epoch 90/100\n",
      "157/157 [==============================] - 21s 131ms/step - total_loss: 9.6093 - reconstruction_loss: 4.4437 - kl_loss: 3.1899\n",
      "Epoch 91/100\n",
      "157/157 [==============================] - 21s 131ms/step - total_loss: 9.5849 - reconstruction_loss: 3.8838 - kl_loss: 3.2694\n",
      "Epoch 92/100\n",
      "157/157 [==============================] - 21s 131ms/step - total_loss: 9.5593 - reconstruction_loss: 4.0240 - kl_loss: 3.2661\n",
      "Epoch 93/100\n",
      "157/157 [==============================] - 21s 133ms/step - total_loss: 9.5352 - reconstruction_loss: 4.1832 - kl_loss: 3.2329\n",
      "Epoch 94/100\n",
      "157/157 [==============================] - 20s 130ms/step - total_loss: 9.5131 - reconstruction_loss: 4.5570 - kl_loss: 3.1700\n",
      "Epoch 95/100\n",
      "157/157 [==============================] - 21s 131ms/step - total_loss: 9.4907 - reconstruction_loss: 4.1157 - kl_loss: 3.2290\n",
      "Epoch 96/100\n",
      "157/157 [==============================] - 21s 131ms/step - total_loss: 9.4741 - reconstruction_loss: 5.2724 - kl_loss: 3.1499\n",
      "Epoch 97/100\n",
      "157/157 [==============================] - 21s 133ms/step - total_loss: 9.4566 - reconstruction_loss: 5.0440 - kl_loss: 3.1385\n",
      "Epoch 98/100\n",
      "157/157 [==============================] - 21s 132ms/step - total_loss: 9.4363 - reconstruction_loss: 4.3527 - kl_loss: 3.1934\n",
      "Epoch 99/100\n",
      "157/157 [==============================] - 21s 133ms/step - total_loss: 9.4216 - reconstruction_loss: 4.7903 - kl_loss: 3.1624\n",
      "Epoch 100/100\n",
      "157/157 [==============================] - 21s 132ms/step - total_loss: 9.3999 - reconstruction_loss: 4.0189 - kl_loss: 3.2558\n"
     ]
    }
   ],
   "source": [
    "history = vae.fit(data,epochs=100,batch_size=64,verbose = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "history_df= pd.DataFrame(history.history)\n",
    "history_df.to_csv(\"300epoch_64_history.csv\")\n",
    "history_df.plot()\n",
    "\n",
    "vae.encoder.save(\"Weights/Encoder_300_64.h5\")\n",
    "vae.decoder.save(\"Wegiths/Decoder_300_64.h5\")\n",
    "vae.encoder.save_weights(\"Weights/ncoder_300_64_weights.h5\")\n",
    "vae.decoder.save_weights(\"Weights/Decode_300_64_weights.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
